

# 要件定義書（Codex投入用）: Twilog Analytics (HTMX + FastAPI)

## 1. 目的

TwilogからエクスポートしたCSVをアップロードし、**投稿の統計分析**と**可視化（グラフ、ヒートマップ、ワードクラウド、単語ランキング等）**をブラウザで閲覧できるWebアプリを提供する。
UIは **HTMXで部分更新**し、フロントSPAは採用しない。 ([Zenn][1])

## 2. ゴール

* CSVアップロード → 自動前処理 → 分析結果（複数タブ/ページ）表示
* 形態素解析は **SudachiPy + sudachidict-full** 固定
* 可視化は **Plotly（インタラクティブ）** と **Matplotlib（静的）** を併用
* データ処理は **Polars** を第一選択（pandasは必要なら補助）
* README添付プロジェクト（Streamlitアプリ）の分析メニュー構成を“Web版”に移植するイメージで、分析数は多めに用意する  

## 3. 非ゴール

* Twitter/X API連携（取得・更新）はしない（CSV入力のみ）
* ログイン、ユーザー管理、多人数共有はしない（ローカル利用前提）
* “いいね/RT数”など外部指標の取得はしない（CSVに無い）

## 4. 想定ユーザー

* 自分の投稿履歴を振り返りたい個人
* 技術メモ・学習ログとして投稿を使っている人

## 5. 入力データ仕様

### 5.1 対応CSV（最低要件）

* Twilog由来CSV（UTF-8想定。ただし文字化け対策で複数エンコーディング試行）
* **ヘッダ有無両対応**
* 最低限、以下を抽出できればよい

  * tweet_id（文字列で保持）
  * created_at（日時にパース）
  * text（本文）
  * status_url（あれば保持）

Twilogの基本形（3列）もあり得るため、列マッピングは自動推定する。 ([Taste of Tech Topics][2])

### 5.2 派生カラム（前処理で生成）

* date（YYYY-MM-DD）
* year / month / weekday / hour
* text_length（文字数）
* url_count（本文中URL数）
* domains（本文中URLのドメイン配列、集計用にexplode）
* hashtag_list / hashtag_count
* mention_list / mention_count
* is_reply（先頭が@）
* contains_code（``` や “import”, “const”, “def” 等の簡易判定）
* language_hint（日本語/英語比率の簡易推定：ASCII率など）

## 6. 機能要件

## 6.1 画面（HTMX前提）

### A. トップ / アップロード

* CSVアップロードフォーム
* 解析オプション

  * 対象年フィルタ（全期間/複数年）
  * Sudachi分割モード（A/B/C）
  * 品詞フィルタ（名詞のみ、名詞+動詞、全トークン）
  * ストップワード（ユーザー追加可能：改行区切り）
* 「解析開始」→ 結果ページへ遷移（またはHTMXで部分差し替え）

### B. ダッシュボード（分析メニュー）

* 左にナビ（分析カテゴリ）
* 右に表示領域（HTMXで部分更新）
* 各分析に「設定パネル（上位Nなど）」と「画像/グラフ」「表（CSVダウンロード）」を用意

> 実装方針：FastAPI + Jinja2テンプレート、`hx-get/hx-post`で部分HTMLを差し替え。 ([Zenn][1])

## 6.2 分析/可視化メニュー（できるだけ多く）

### Ⅰ. 投稿量・時系列（基本統計）

1. 総投稿数、期間（最古/最新）、1日平均
2. 年別投稿数（棒 or 折れ線）
3. 月別投稿数（年フィルタ対応）
4. 週別（ISO week）投稿数
5. 曜日別投稿数（棒）
6. 時間帯別投稿数（0-23）
7. 曜日×時間ヒートマップ
8. 日別投稿数カレンダー（Plotlyのcalendar heatmap風）
9. 投稿間隔（隣接ツイート差分）の分布（ヒストグラム）
10. “連投セッション”検出（例：30分以内を同セッション）→ セッション長分布
11. 深夜投稿比率（例：0-5時）
12. 月内の投稿日（1-31日）の偏り

### Ⅱ. テキスト量・形式

13. 文字数分布（ヒストグラム）
14. 長文ランキング（上位N：tweet_idリンク付き）
15. URL含有率（URLあり/なし比率）
16. URL数分布（1投稿あたり）
17. ハッシュタグ数分布
18. メンション数分布
19. リプライ比率（@開始）
20. 改行数分布（複数行投稿の傾向）
21. 絵文字数分布（簡易：Unicodeレンジ）

### Ⅲ. リンク/ドメイン分析（本文URL抽出）

22. ドメイン別出現数（上位N）
23. TLD別（.com/.dev/.jp 等）分布
24. ドメイン×年（年別に上位ドメイン推移）
25. ドメイン×月トレンド（特定ドメインを選んで折れ線）
26. URLの “twitter.com/x.com 参照” 比率（自己参照/外部参照）
27. 外部リンクのパス深さ分布（/が何個か）

### Ⅳ. ハッシュタグ/メンション

28. ハッシュタグランキング（上位N）
29. ハッシュタグ共起（同一投稿内で一緒に出るタグ）→ ネットワーク図（簡易でも可）
30. メンション先ランキング（上位N）
31. メンション×曜日（交流が多い曜日）
32. ハッシュタグ×年推移（タグの流行）

### Ⅴ. 形態素解析（SudachiPy固定）

33. ワードクラウド（名詞のみ）
34. ワードクラウド（名詞+動詞）
35. 単語ランキング（上位N、出現回数）
36. 単語ランキング（TF-IDF：年 or 月を“文書”として扱う）
37. 単語の時系列（選択単語の月次出現）
38. 共起語ランキング（指定語と同投稿内に出る語）
39. 共起ネットワーク（上位語、閾値でエッジ）
40. Sudachi分割モードA/B/C比較（同じデータで上位語がどう変わるか）

※ツイートのテキストマイニングで、形態素解析→ワードクラウド、共起などは定番の方向性。 ([Qiita][3])

### Ⅵ. トピック/クラスタ（軽量に）

41. 月ごとの代表語（TF-IDF上位語を月別に表）
42. 投稿ベクトル化（TF-IDF）→ KMeansでクラスタ → クラスタ別代表語
43. トピック簡易表示（LDAは重いので“任意・後回し”でもOK。まずはTF-IDFで十分）

### Ⅶ. “技術メモ”向けの追加（あなたのCSV傾向に合わせる）

44. 「Cloudflare / Next.js / Hono / Auth」等の**キーワード辞書**を用意し、カテゴリ別投稿数（辞書はユーザー編集可）
45. コード断片っぽい投稿検出（contains_code）→ 時系列
46. “自分の過去ツイートURL”を含む投稿の割合（スレッド/自己参照傾向）

## 6.3 出力・ダウンロード

* 主要な集計表はCSVでダウンロード可能（例：単語ランキング、ドメインランキング）
* グラフはPNG保存（サーバ側生成 or フロントで画像化のどちらでも可。まずはサーバ生成）

## 7. 非機能要件

* ローカル実行（`uv run` で起動）
* 数千〜数万ツイートでも耐える設計（Polars、キャッシュ）
* 文字化けに強い（複数エンコーディング試行、失敗時にエラー表示）
* セキュリティ：アップロードファイルはメモリor一時ディレクトリ、パストラバーサル禁止
* パフォーマンス：解析結果をセッション単位でキャッシュ（同一ファイル再解析を避ける）

## 8. 技術スタック・依存（添付プロジェクトに寄せる）

* Python >= 3.12
* polars
* matplotlib / plotly / seaborn（seabornは必須ではないが依存は合わせる）
* matplotlib-fontja（日本語フォント）
* sudachipy + sudachidict-full（形態素解析）
* wordcloud + pillow
* 追加：fastapi / uvicorn / jinja2（SSR）/ python-multipart（upload）/ (任意) orjson

※ベース依存は添付 `pyproject.toml` を踏襲すること。 
（Streamlitは使わないが、分析ロジック/可視化ロジックは流用しやすい構造にする） 

## 9. アーキテクチャ要件（推奨ディレクトリ）

添付プロジェクトの分割（data / analysis / visualization）を踏襲しつつ、Web層を追加：

```
twilog-analytics/
├── app/
│   ├── main.py                 # FastAPI entry
│   ├── routes/
│   ├── templates/              # Jinja2 (base, dashboard, partials)
│   └── static/                 # css, optional js
├── src/twilog_analytics/
│   ├── data/
│   │   ├── loader.py           # CSV読込・エンコーディング推定・列推定
│   │   └── preprocessor.py     # 派生カラム生成
│   ├── analysis/
│   │   ├── statistics.py
│   │   ├── timeseries.py
│   │   ├── link_analysis.py
│   │   └── text_analysis.py    # SudachiPy
│   └── visualization/
│       ├── plotly_charts.py
│       ├── matplotlib_charts.py
│       └── wordcloud_viz.py
└── AGENTS.md
```

## 10. ルーティング（例）

* `GET /` アップロード画面
* `POST /upload` 解析開始（ファイル保存→前処理→セッションにキー保存→`/dashboard`へ）
* `GET /dashboard` 初期ダッシュボード
* `GET /partials/{analysis_name}` 各分析のHTML断片（HTMXで差し替え）
* `GET /download/{kind}.csv` 集計CSV返却

## 11. 受け入れ条件（Definition of Done）

* サンプルCSV（添付）でエラーなく読み込みでき、最低でも以下が表示される

  * 年/月/曜日/時間の投稿傾向
  * URLドメイン上位
  * ワードクラウド（Sudachi）
  * 単語ランキング表（DL可能）
* UIはページ全体のリロード無しに、分析の切替がHTMXで部分更新される ([Zenn][1])
* `uv run` で起動できる（READMEに手順）

## 12. Codexへの実装指示（プロンプトに含める運用ルール）

* まず `AGENTS.md` を作成し、命名・コマンド・構造・依存を固定する（以後の作業で参照）
* 1タスクは「1時間程度の変更単位」に分割し、Askモードで計画→実装へ 
* 既存（添付）プロジェクトの分析モジュール構造に寄せ、再利用できる粒度で実装 
